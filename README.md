# JustNewsAgentic V4

This project implements the JustNews V4 system, an agentic, MCP-first news analysis ecosystem with **Native TensorRT GPU acceleration**. The system is designed as a collaborative group of specialized AI agents that work together to find, analyze, and synthesize news stories with professional-grade GPU acceleration.

## üéØ **MAJOR BREAKTHROUGH - Production-Scale News Crawling Operational** 

**Latest Update**: August 2, 2025 - **Production-scale BBC crawling with root cause resolution**

### üöÄ **Production BBC Crawler - ‚úÖ BREAKTHROUGH ACHIEVED**
- **Performance**: **8.14 articles/second** with ultra-fast processing (700K+ articles/day capacity)
- **Quality**: **0.86 articles/second** with full AI analysis (74K+ articles/day capacity)  
- **Success Rate**: **95.5%** successful content extraction with real news content
- **Root Cause Resolution**: Cookie consent and modal handling completely solved
- **Content Quality**: Real BBC news extraction (murders, arrests, government announcements)

### ÔøΩ **Model Loading Issues - ‚úÖ COMPLETELY RESOLVED**
- **Problem**: LLaVA model warnings and type mismatches causing potential errors
- **Solution**: Corrected processor/model combinations (`LlavaProcessor` + `LlavaForConditionalGeneration`)
- **Result**: Clean model loading with `use_fast=True` and no warnings
- **Impact**: Stable foundation for production news analysis at scale

### üï∑Ô∏è **Web Scraping Breakthrough - ‚úÖ COOKIE WALL DEFEATED**
- **Insight**: Cookie consent and JavaScript modals were root cause of both crashes AND content failure
- **Method**: Aggressive modal dismissal with DOM-based content extraction
- **Performance**: Bypassed BBC cookie walls to extract real article content
- **Scalability**: Multi-browser concurrent processing for maximum throughput

### ü§ñ **NewsReader Integration - ‚úÖ PRODUCTION STABLE + SCOUT INTEGRATION**
- **Model**: LLaVA-1.5-7B with INT8 quantization (6.8GB GPU memory)
- **Processing**: Screenshot analysis and DOM extraction hybrid approach
- **Stability**: Zero crashes with proper memory management and modal handling
- **Output**: Real news analysis with article titles, content, and metadata
- **Scout Enhancement**: Enhanced crawling with visual + text content analysis
- **MCP Integration**: Full agent communication via port 8009 with comprehensive API

## üéØ Previous Achievements - Scout ‚Üí Memory Pipeline

### ÔøΩüöÄ **Scout Agent Content Extraction - ‚úÖ PRODUCTION READY**
- **Method**: Enhanced `cleaned_html` extraction with intelligent article filtering
- **Performance**: **1,591 words** of clean article content per extraction
- **Quality**: Smart navigation filtering removes BBC menus, headers, and promotional content
- **Source**: `enhanced_deepcrawl_main_cleaned_html` with 30.5% extraction efficiency
- **Integration**: Full MCP Bus communication with native TensorRT support

### üîÑ **MCP Bus Communication - ‚úÖ FULLY OPERATIONAL**
- **Agent Registration**: Scout and Memory agents properly registered
- **Tool Routing**: Complete request/response cycle working
- **Native Deployment**: All Docker dependencies removed for production performance
- **Background Services**: Robust daemon startup with health checks and timeouts

### üíæ **Memory Agent Integration - ‚úÖ DATABASE CONNECTED**
- **PostgreSQL**: Native connection established with user authentication
- **Schema**: Articles, article_vectors, training_examples tables operational  
- **API Compatibility**: Hybrid endpoint handling for both direct calls and MCP Bus format
- **Status**: Database connection working, final serialization fix in progress

### üß† **Scout Intelligence Engine - ‚úÖ GPU ACCELERATED**
- **Model**: LLaMA-3-8B GPU-accelerated content analysis
- **Performance**: Native TensorRT integration for 4x speed improvement
- **Quality Analysis**: Real-time content scoring and article indicator detection
- **Integration**: Seamless MCP Bus communication with sub-second response times

## üéØ Memory Optimization Status - ‚úÖ MISSION ACCOMPLISHED

**Previous Achievement**: July 29, 2025 - **Production deployment successful**

### Memory Crisis Resolved
- **Problem**: RTX 3090 memory exhaustion (-1.3GB buffer) blocking production
- **Solution**: Strategic Phase 1 optimizations deployed with intelligence-first architecture  
- **Result**: **6.4GB memory savings**, **5.1GB production buffer** ‚úÖ (exceeds 3GB target by 67%)
- **Status**: **Production-ready** with automated deployment tools and backup procedures

### Strategic Architecture Achievement
**Intelligence-First Design**: Scout pre-filtering enables downstream optimization
- **Fact Checker**: DialoGPT-large ‚Üí medium (2.7GB saved) - Scout pre-filtering compensates
- **Synthesizer**: Lightweight embeddings + context optimization (1.5GB saved)
- **Critic**: Context and batch optimization (1.2GB saved)  
- **Chief Editor**: Orchestration optimization (1.0GB saved)
- **Total Impact**: 23.3GB ‚Üí 16.9GB usage with robust production buffer

### Deployment Status
‚úÖ **4/4 agents optimized** and validated  
‚úÖ **GPU confirmed ready**: RTX 3090 with 23.5GB available  
‚úÖ **Backup complete**: Automatic rollback capability implemented
‚úÖ **Production safe**: Conservative optimizations with comprehensive validation

**Complete Results**: See `DEPLOYMENT_SUCCESS_SUMMARY.md` for detailed deployment report

## ÔøΩ Enhanced Scout Agent - Native Crawl4AI Integration

**Latest Achievement**: Scout agent now features native Crawl4AI integration with BestFirstCrawlingStrategy for advanced web crawling capabilities (July 29, 2025)

### üöÄ **Enhanced Deep Crawl System**
- ‚úÖ **Native Crawl4AI Integration**: Version 0.7.2 with BestFirstCrawlingStrategy
- ‚úÖ **Scout Intelligence Analysis**: LLaMA-3-8B content quality assessment and filtering
- ‚úÖ **Quality Threshold Filtering**: Configurable quality scoring with smart content selection
- ‚úÖ **User-Configurable Parameters**: max_depth=3, max_pages=100, word_count_threshold=500
- ‚úÖ **MCP Bus Communication**: Full integration with inter-agent messaging system

### üìä **Enhanced Deep Crawl Features**
- **BestFirstCrawlingStrategy**: Intelligent crawling prioritizing high-value content
- **FilterChain Integration**: ContentTypeFilter and DomainFilter for focused crawling
- **Scout Intelligence**: Comprehensive content analysis with bias detection and quality metrics
- **Quality Scoring**: Dynamic threshold-based filtering for high-quality content selection
- **Fallback System**: Automatic Docker fallback for reliability and compatibility

### üß† **Scout Intelligence Engine**
- **GPU-Accelerated Analysis**: LLaMA-3-8B model for content quality assessment
- **Comprehensive Analysis**: News classification, bias detection, quality metrics
- **Quality Filtering**: Smart threshold-based content selection
- **Performance Optimized**: Batch processing with efficient GPU utilization

### üîß **Usage Example**
```python
# Enhanced deep crawl with user parameters
results = await enhanced_deep_crawl_site(
    url="https://news.sky.com",
    max_depth=3,                    # User requested
    max_pages=100,                  # User requested  
    word_count_threshold=500,       # User requested
    quality_threshold=0.05,         # Configurable
    analyze_content=True            # Scout Intelligence enabled
)
```

### üéØ **Performance Validation**
- **Integration Test Results**: Successfully crawled Sky News (148k characters, 1.3s)
- **Scout Intelligence Applied**: Content analysis with score 0.10, quality filtering operational
- **MCP Bus Communication**: Full integration with agent registration and messaging
- **Quality System**: Smart filtering with configurable thresholds for production use

## ÔøΩüèÜ V4 Production Status: GPU-Accelerated System OPERATIONAL

**Latest Achievement**: Production-scale GPU deployment validated with 1,000 full-length articles - CUDA device management optimized for crash-free operation (July 28, 2025)

### üéØ **Production Performance Validated**
- ‚úÖ **Sentiment Analysis**: 151.4 articles/sec (75.7% of V4 target, 2,717-char articles)
- ‚úÖ **Bias Analysis**: 146.8 articles/sec (73.4% of V4 target, production-scale content)
- ‚úÖ **System Stability**: 1,000-article stress test completed without crashes
- ‚úÖ **CUDA Optimization**: Professional device management eliminates GPU/CPU tensor conflicts
- ‚úÖ **Memory Efficiency**: Water-cooled RTX 3090 utilization optimized with FP16 precision

### ÔøΩ **Production-Ready Architecture**
## Performance Metrics (Production Validated)

### Native TensorRT Performance (RTX 3090 - PRODUCTION VALIDATED ‚úÖ)
**Current Status**: ‚úÖ **PRODUCTION STRESS TESTED** - 1,000 articles √ó 2,000 chars successfully processed

**Validated Performance Results** (Realistic Article Testing):
- **Sentiment Analysis**: **720.8 articles/sec** (production validated with 2,000-char articles)
- **Bias Analysis**: **740.3 articles/sec** (production validated with 2,000-char articles)
- **Combined Average**: **730+ articles/sec** sustained throughput
- **Total Processing**: 1,000 articles (1,998,208 characters) in 2.7 seconds
- **Reliability**: 100% success rate, zero errors, zero timeouts
- **Memory Efficiency**: 2.3GB GPU utilization (efficient resource usage)
- **Stability**: Zero crashes, zero warnings under production stress testing

**Baseline Comparison**:
- **HuggingFace GPU Baseline**: 151.4 articles/sec
- **Native TensorRT Production**: 730+ articles/sec
- **Improvement Factor**: **4.8x** (exceeding V4 target of 3-4x)

### System Architecture Status
- ‚úÖ **Native TensorRT Integration**: Production-ready with FP16 precision
- ‚úÖ **CUDA Context Management**: Professional-grade resource handling
- ‚úÖ **Batch Processing**: Optimized 100-article batches
- ‚úÖ **Memory Management**: Efficient GPU memory allocation and cleanup
- ‚úÖ **Fallback System**: Automatic CPU fallback for reliability

### üéØ Current Phase: Production Deployment Ready
- **Phase 1 Complete**: Native TensorRT production validation achieved  
- **Performance Validated**: 730+ articles/sec with realistic 2,000-character articles
- **Stress Testing**: Successfully processed 1,000 articles with 100% reliability
- **Production Ready**: Zero errors, optimal GPU utilization, professional CUDA management
- **Next Actions**: Deploy to production, expand to remaining agents, apply native pattern

### üîÑ V4 Migration Status
- **Current**: V3.5 architecture achieving V4 performance targets
- **Next Phase**: RTX AI Toolkit integration (TensorRT-LLM, AIM SDK, AI Workbench)
- **Performance Maintained**: Migration will preserve current speeds while adding V4 features

### ‚è≥ Pending V4 Integration (Ready for Implementation)
- **TensorRT-LLM**: Installed and configured, awaiting pipeline integration
- **AIM SDK**: Configuration ready, awaiting NVIDIA developer access
- **AI Workbench**: QLoRA fine-tuning pipeline for domain specialization
- **RTXOptimizedHybridManager**: Architecture designed, awaiting implementation

## Architecture

JustNews V4 features a **multi-agent news analysis system** with **native TensorRT GPU acceleration** and **MCP (Model Context Protocol) bus communication**. The architecture consists of 10 specialized agents communicating through a central message bus.

**Current Status**: Native TensorRT Production Deployment with Enhanced NewsReader Integration - validated with full pipeline testing achieving **complete visual + text content analysis** with zero crashes and completely clean operation.

### Agent Specifications (Optimized for RTX 3090)

| Agent | Model | Memory | Status | Performance |
|-------|-------|---------|--------|-------------|
| **Analyst** | RoBERTa + BERT (TensorRT) | 2.3GB | ‚úÖ Production | 730+ articles/sec |
| **Scout** | LLaMA-3-8B + Crawl4AI | 8.0GB | ‚úÖ Enhanced Deep Crawl + NewsReader | Native BestFirstCrawlingStrategy |
| **NewsReader** | LLaVA-1.5-7B (INT8) | 6.8GB | ‚úÖ Production | Screenshot + Visual Analysis |
| **Fact Checker** | DialoGPT-medium | 2.5GB | ‚è≥ TensorRT Ready | Scout-Optimized |
| **Synthesizer** | DialoGPT-medium + Embeddings | 3.0GB | ‚è≥ TensorRT Ready | Content Synthesis |
| **Critic** | DialoGPT-medium | 2.5GB | ‚è≥ TensorRT Ready | Quality Assessment |
| **Chief Editor** | DialoGPT-medium | 2.0GB | ‚è≥ TensorRT Ready | Orchestration |
| **Memory** | Vector Embeddings | 1.5GB | ‚è≥ TensorRT Ready | Semantic Search |
| **Reasoning** | Nucleoid (symbolic logic) | <1GB | ‚úÖ Production | Fact validation, contradiction detection |
| **Total System** | **Multi-Model Pipeline** | **29.6GB** | **RTX 3090 Optimized** | **Requires Optimization** |

### Strategic Architecture Design

**Intelligence-First Pipeline**: Scout agent with LLaMA-3-8B performs ML-based content pre-filtering, removing opinion pieces, forum discussions, and non-news content. This allows downstream agents to use smaller, more efficient models while maintaining accuracy.

**Enhanced Deep Crawling**: Scout agent now features native Crawl4AI integration with BestFirstCrawlingStrategy for advanced web crawling with user-configurable parameters (max depth 3, max pages 100, word count threshold 500). The enhanced deep crawl system combines intelligent crawling strategies with Scout Intelligence analysis for quality-filtered content discovery.

**Native TensorRT Acceleration**: Production-validated TensorRT engines deliver 4.8x performance improvements with professional CUDA memory management and zero-crash reliability.

### Core Components
- **MCP Bus** (Port 8000): Central communication hub using FastAPI with `/register`, `/call`, `/agents` endpoints
- **Agents** (Ports 8001-8008): Independent FastAPI services (GPU/CPU)
- **Enhanced Scout Agent**: Native Crawl4AI integration with BestFirstCrawlingStrategy and Scout Intelligence analysis
- **Reasoning Agent**: Complete Nucleoid GitHub implementation with AST parsing, NetworkX dependency graphs, symbolic reasoning, fact validation, and contradiction detection (Port 8008)
- **Database**: PostgreSQL + vector search for semantic article storage
- **GPU Stack**: Water-cooled RTX 3090 with native TensorRT 10.10.0.31, PyCUDA, professional CUDA management

**V4 RTX Architecture**: JustNews V4 introduces GPU-accelerated news analysis with current V3.5 implementation patterns achieving V4 performance targets. Full RTX AI Toolkit integration (TensorRT-LLM, AIM SDK, AI Workbench) planned for Phase 2 migration while maintaining current performance levels.

For full architectural details, see:
- **V4 (Current)**: `docs/JustNews_Proposal_V4.md` and `docs/JustNews_Plan_V4.md`
- **V3 (Legacy)**: `docs/JustNews_Proposal_V3.md` and `docs/JustNews_Plan_V3.md`


## üöÄ **Service Management - Native Deployment** 

**Status**: Full native deployment operational with background daemon services

### Start System
```bash
# Start all agents as background daemons
./start_services_daemon.sh

# Services will start in order:
# 1. MCP Bus (port 8000) - Central coordination hub
# 2. Scout Agent (port 8002) - Content extraction with Crawl4AI
# 3. Memory Agent (port 8007) - PostgreSQL database storage
# 4. Reasoning Agent (port 8008) - Symbolic reasoning, fact validation
```

### Stop System  
```bash
# Graceful shutdown with proper cleanup
./stop_services.sh

# Kills all background processes and cleans up PIDs
```

### Service Status
```bash
# Check all services
ps aux | grep -E "(mcp_bus|scout|memory|reasoning)" | grep -v grep

# Current active services:
# ‚úÖ MCP Bus: PID 20977 on port 8000 (Request routing)
# ‚úÖ Scout Agent: PID 20989 on port 8002 (Content extraction)  
# ‚úÖ Memory Agent: PID 20994 on port 8007 (Database storage)
# ‚úÖ Reasoning Agent: PID XXXXX on port 8008 (Symbolic reasoning)
```

### Health Check
```bash
# Verify MCP Bus and agent registration
curl http://localhost:8000/agents

# Expected response:
# {
#   "agents": {
#     "scout": {"url": "http://localhost:8002", "status": "registered"},
#     "memory": {"url": "http://localhost:8007", "status": "registered"}
#   }
# }
```

## üî¨ **Pipeline Testing Results**

### Scout Agent ‚Üí Memory Agent Pipeline ‚úÖ FUNCTIONAL

**Latest Test Results** (test_full_pipeline_updated.py):
```
‚úÖ Scout Agent Response:
   Title: "Two hours of terror in a New York skyscraper - BBC News"
   Content: 1,591 words (9,612 characters)
   Method: enhanced_deepcrawl_main_cleaned_html  
   URL: https://www.bbc.com/news/articles/c9wj9e4vgx5o
   Quality: 30.5% extraction efficiency (removes BBC navigation/menus)

‚úÖ Memory Agent Communication:
   Request Format: {"args": [url], "kwargs": {}}
   Response: "Request received successfully"
   Database: PostgreSQL connection established
   Status: ‚úÖ Ready for article storage (dict serialization fix in progress)
```

**Content Quality Example** (Sample Extract):
```
"Marcus Moeller had just finished a presentation at his law firm on the 39th floor...
...spanning two hours of terror that ended only when heavily armed tactical officers
stormed the building and killed the gunman..."
```
- **Clean Extraction**: No BBC menus, navigation, or promotional content
- **Readable Format**: Proper paragraph structure maintained  
- **Article Focus**: Pure news content with context preserved

## Getting Started

### Prerequisites

- **NVIDIA Hardware**: RTX 3090 (24GB VRAM) with water cooling recommended for sustained performance
- **Ubuntu 24.04 Native**: Direct GPU access for optimal performance (40-110% improvement over WSL2)
- **Conda Environment**: Python 3.12 with PyTorch 2.2.0+cu121 and transformers ecosystem
- **GPU Memory**: Minimum 16GB VRAM (24GB recommended for multi-agent deployment)
- **System Requirements**: 32GB+ RAM, NVMe SSD for model caching

### üöÄ Production Environment Setup

The V4 system runs on a validated GPU-optimized environment:

1. **Conda Environment Setup (Validated)**:
   ```bash
   # Activate the production-ready environment
   source /home/adra/miniconda3/etc/profile.d/conda.sh
   conda activate rapids-25.06
   
   # Verify GPU integration
   python -c "import torch; print(f'‚úÖ CUDA: {torch.cuda.is_available()}')"
   python -c "import transformers; print('‚úÖ Transformers Ready!')"
   ```

2. **Production Environment Specifications (VALIDATED)**:
   ```yaml
   Environment: rapids-25.06
   Python: 3.12
   CUDA Toolkit: 12.1
   
   Core GPU Stack:
   - torch: 2.2.0+cu121
   - torchvision: 0.17.0+cu121
   - transformers: 4.39.0
   - sentence-transformers: 2.6.1
   - numpy: 1.26.4 (compatibility fix)
   
   System Requirements:
   - NVIDIA Driver: 550+ (water-cooled RTX 3090)
   - Memory: 32GB+ RAM, 24GB+ VRAM
   - Storage: NVMe SSD for model caching
   ```

3. **GPU Service Deployment**:
   ```bash
   # Start production GPU analyst service
   python start_native_gpu_analyst.py
   
   # Verify service health
   curl -s http://localhost:8004/health | jq .
   ```

4. **Production Validation**:
   ```bash
   # Run production stress test
   python production_stress_test.py
   
   # Expected results: 151.4 art/sec sentiment, 146.8 art/sec bias
   # GPU status monitoring
   nvidia-smi
   ```

4. **Install NVIDIA Container Toolkit (Windows WSL2 or Linux):**
   ```bash
   # Follow: https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html
   ```

### Build & Run

1. **GPU-Accelerated Environment (WSL2)**:
   ```bash
   # Activate the RAPIDS environment with TensorRT-LLM
   source /home/nvidia/.venvs/rapids25.06_python3.12/bin/activate
   
   # Launch the hybrid V4 system
   cd /mnt/c/Users/marti/JustNewsAgentic/wsl_deployment
   python main.py
   ```

2. **Docker Multi-Agent System (5 agents)**:
    ```bash
    # From Windows PowerShell or WSL
    docker-compose up --build
    ```

3. **System Components**:
    - **GPU-Accelerated (WSL Native)**:
      - `analyst`: GPU batch processing (5.7 articles/sec) - sentiment, bias, entity analysis with TensorRT-LLM/RAPIDS
    
    - **Docker-Based**:
      - `mcp_bus`: Central message bus for agent communication
      - `chief_editor`: Orchestrates news workflows
      - `scout`: Discovers and crawls news sources  
      - `fact_checker`: Validates news and verifies claims
      - `synthesizer`: Clusters and synthesizes articles using ML models
      - `critic`: Reviews synthesis and neutrality using LLM-based critique
      - `memory`: Unified data access (PostgreSQL, vector search)
      - `db`: PostgreSQL database

4. **Performance Expectations**:
   - **Current (WSL2)**: 5.7 articles/sec GPU batch processing
   - **Ubuntu Native**: 8-12 articles/sec expected (40-110% improvement)

### Database Setup & Migrations


Database migrations are located in `agents/memory/db_migrations/`. To apply them manually (from the project root):

```bash
docker-compose exec db psql -U user -d justnews -f /app/db_migrations/001_create_articles_table.sql
docker-compose exec db psql -U user -d justnews -f /app/db_migrations/002_create_training_examples_table.sql
docker-compose exec db psql -U user -d justnews -f /app/db_migrations/003_create_article_vectors_table.sql
```


### ML, Feedback Loops & Continual Learning

All major agents implement machine learning-based feedback loops for continual improvement:

- **Synthesizer**: Uses sentence-transformers for clustering (KMeans, BERTopic, HDBSCAN), LLM for neutralization/aggregation, and logs feedback from Critic/Chief Editor for continual learning. Clustering method is set via `SYNTHESIZER_CLUSTER_METHOD`.
- **Critic**: Uses LLM for critique, logs all feedback and editorial outcomes for retraining and adaptation. Optional fact-checking pipeline can be enabled with `CRITIC_USE_FACTCHECK=1`.
- **Memory**: Provides semantic retrieval with embeddings and vector search, logs all retrievals and downstream outcomes for future learning-to-rank and model improvement. Tool interfaces are mirrored in `tools.py` for clarity and local testing.

**Feedback Logging:**
- All agents log feedback to agent-specific files (e.g., `./feedback_synthesizer.log`) and/or the database.
- Feedback includes tool usage, outcomes, errors, and user/editorial input.
- Standardized logging format: UTC timestamp, event, details (as JSON/dict).

**Retraining & Continual Learning:**
- Feedback logs are used for both online and scheduled retraining.
- Retraining procedures are documented in each agent's code and can be automated via scripts or CI/CD.
- See `action_plan.md` and agent `tools.py` for retraining hooks and feedback usage.

### API Endpoints

Each agent exposes its tools as HTTP endpoints (see `main.py` in each agent folder). The MCP bus provides `/register`, `/call`, and `/agents` endpoints for agent discovery and tool invocation.

### Docker GPU Troubleshooting

**GPU Access Issues:**
- Ensure NVIDIA Container Toolkit is installed and configured
- Verify Docker can access GPU: `docker run --rm --gpus all nvidia/cuda:11.8-base-ubuntu20.04 nvidia-smi`
- Check GPU resources in Docker Desktop settings (increase memory limits)

**Model Loading Issues:**
- Verify model files exist: `ls -la ~/mistral_models/7B-Instruct-v0.3/`
- Check volume mounting in docker-compose logs
- For Windows, copy `docker-compose.override.example.yml` to `docker-compose.override.yml` and adjust paths

**Memory Issues:**
- Analyst agent requires ~14.5GB VRAM for optimal performance
- If OOM errors occur, check available GPU memory: `nvidia-smi`
- Consider reducing batch size or using CPU fallback by removing GPU config

### Dual Functionality: Standalone Execution & MCP Bus Integration

Each agent can be started independently while maintaining the ability to communicate with other agents via the MCP Bus. Follow the instructions below for standalone execution and MCP Bus integration:

#### Standalone Execution
Each agent can operate independently without relying on other agents or services. See the "Standalone Execution" section above for detailed instructions.

#### MCP Bus Integration
When the MCP Bus is available, agents will automatically register their tools and use the bus for inter-agent communication. This allows for dynamic collaboration between agents.

**Key Features:**
- Agents register their tools with the MCP Bus upon startup.
- If the MCP Bus is unavailable, agents will continue to operate independently.
- Error handling ensures smooth operation in standalone mode.

**Starting the MCP Bus:**
1. Navigate to the `mcp_bus` directory.
2. Install dependencies:
    ```bash
    pip install -r requirements.txt
    ```
3. Start the MCP Bus:
    ```bash
    uvicorn main:app --reload --port 8000
    ```

**Agent Registration:**
Agents will automatically attempt to register their tools with the MCP Bus at `http://localhost:8000`. Ensure the MCP Bus is running before starting agents for full functionality.

### Standalone Execution

Each agent can be started independently without relying on other agents or services. Follow the instructions below for standalone execution:

#### Chief Editor Agent
1. Navigate to the `agents/chief_editor` directory.
2. Install dependencies:
    ```bash
    pip install -r requirements.txt
    ```
3. Start the agent:
    ```bash
    uvicorn main:app --reload --port 8001
    ```

#### Scout Agent
1. Navigate to the `agents/scout` directory.
2. Install dependencies:
    ```bash
    pip install -r requirements.txt
    ```
3. Start the agent:
    ```bash
    uvicorn main:app --reload --port 8002
    ```

#### Fact-Checker Agent
1. Navigate to the `agents/fact_checker` directory.
2. Install dependencies:
    ```bash
    pip install -r requirements.txt
    ```
3. Start the agent:
    ```bash
    uvicorn main:app --reload --port 8003
    ```

#### Synthesizer Agent
1. Navigate to the `agents/synthesizer` directory.
2. Install dependencies:
    ```bash
    pip install -r requirements.txt
    ```
3. Start the agent:
    ```bash
    uvicorn main:app --reload --port 8005
    ```

#### Critic Agent
1. Navigate to the `agents/critic` directory.
2. Install dependencies:
    ```bash
    pip install -r requirements.txt
    ```
3. Start the agent:
    ```bash
    uvicorn main:app --reload --port 8006
    ```

#### Memory Agent
1. Navigate to the `agents/memory` directory.
2. Install dependencies:
    ```bash
    pip install -r requirements.txt
    ```
3. Start the agent:
    ```bash
    uvicorn main:app --reload --port 8007
    ```

#### Reasoning Agent
**Status**: ‚úÖ **PRODUCTION READY** - Complete GitHub Implementation Integrated

1. Navigate to the `agents/reasoning` directory.
2. Install dependencies:
    ```bash
    pip install -r requirements.txt
    ```
3. Start the agent:
    ```bash
    uvicorn main:app --reload --port 8008
    ```

**Features**:
- **Complete Nucleoid Implementation**: Full GitHub repository integration with AST parsing
- **Advanced Logic Operations**: Variable assignments, mathematical expressions, dependency tracking
- **Graph-based Dependencies**: NetworkX-powered relationship mapping between variables
- **Contradiction Detection**: Sophisticated logical consistency checking
- **Production Integration**: MCP bus communication, comprehensive test coverage

## üìö Documentation

### Organized Documentation Structure
All detailed project documentation has been organized into the `markdown_docs/` directory for better navigation:

- **üìÅ `/markdown_docs/production_status/`** - Production deployment status and achievement reports
- **üìÅ `/markdown_docs/agent_documentation/`** - Agent-specific implementation guides  
- **üìÅ `/markdown_docs/development_reports/`** - Technical analysis and validation reports
- **üìÑ `/markdown_docs/DEVELOPMENT_CONTEXT.md`** - Complete development history and context

### Key Documentation Files
- **Production Status**: See `markdown_docs/production_status/PRODUCTION_DEPLOYMENT_STATUS.md`
- **Development History**: See `markdown_docs/DEVELOPMENT_CONTEXT.md`
- **Agent Guides**: See `markdown_docs/agent_documentation/` for agent-specific documentation
- **Technical Reports**: See `markdown_docs/development_reports/` for detailed analysis

### Quick Access
- **üìñ Full Documentation Index**: [`markdown_docs/README.md`](markdown_docs/README.md)
- **üìä Latest Production Status**: [`markdown_docs/production_status/PRODUCTION_SUCCESS_SUMMARY.md`](markdown_docs/production_status/PRODUCTION_SUCCESS_SUMMARY.md)
- **üîß Development Context**: [`markdown_docs/DEVELOPMENT_CONTEXT.md`](markdown_docs/DEVELOPMENT_CONTEXT.md)

---

*For the most current development status and detailed technical documentation, see the organized documentation in the `markdown_docs/` directory.*
    ```