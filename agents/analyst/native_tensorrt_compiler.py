#!/usr/bin/env python3
"""
Native TensorRT Compiler for JustNews V4
Converts HuggingFace models to optimized TensorRT engines for maximum GPU performance

Features:
- ONNX conversion with dynamic shape support
- FP16/INT8 quantization for RTX 3090 optimization
- Batch processing optimization
- Production-ready error handling
"""

import os
import sys
import json
import logging
import time
from pathlib import Path
from typing import Dict, Optional, List, Any

# Core ML libraries
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

# Add the current directory to Python path for imports
current_dir = Path(__file__).parent
sys.path.append(str(current_dir))

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class NativeTensorRTCompiler:
    """
    Native TensorRT engine compiler for maximum performance
    Converts HuggingFace models to optimized TensorRT engines
    """
    
    def __init__(self):
        self.engine_dir = Path(__file__).parent / "tensorrt_engines"
        self.engine_dir.mkdir(exist_ok=True)
        self.compiled_engines = {}
        
        # Performance optimization settings
        self.optimization_config = {
            'max_batch_size': 100,
            'max_sequence_length': 1024,
            'optimal_sequence_length': 512,
            'precision': 'fp16',  # fp16, int8, fp32
            'workspace_size': 2 << 30,  # 2GB workspace for complex optimizations
        }
        
        logger.info("🔥 Initializing Native TensorRT Compiler")
        self._validate_environment()
    
    def _validate_environment(self):
        """Validate TensorRT environment and GPU capabilities"""
        try:
            import tensorrt as trt
            logger.info(f"✅ TensorRT version: {trt.__version__}")
            
            # Check GPU capabilities
            import torch
            if torch.cuda.is_available():
                gpu_name = torch.cuda.get_device_name(0)
                gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
                logger.info(f"✅ GPU: {gpu_name} ({gpu_memory:.1f}GB)")
                
                # Check for Tensor Cores (RTX series)
                if 'RTX' in gpu_name or 'Tesla' in gpu_name or 'V100' in gpu_name:
                    logger.info("✅ Tensor Cores detected - FP16 optimization enabled")
                    self.tensor_cores_available = True
                else:
                    logger.warning("⚠️  No Tensor Cores detected - using FP32")
                    self.optimization_config['precision'] = 'fp32'
                    self.tensor_cores_available = False
            else:
                raise RuntimeError("No CUDA GPU available")
                
        except ImportError as e:
            raise RuntimeError(f"TensorRT not available: {e}")
        except Exception as e:
            raise RuntimeError(f"Environment validation failed: {e}")
    
    def compile_sentiment_model(self) -> bool:
        """Compile RoBERTa sentiment model to native TensorRT engine"""
        logger.info("🔧 Compiling RoBERTa sentiment model to native TensorRT")
        
        model_name = "cardiffnlp/twitter-roberta-base-sentiment-latest"
        engine_path = self.engine_dir / "native_sentiment_roberta.engine"
        
        try:
            # Step 1: Convert HuggingFace model to ONNX
            onnx_path = self._convert_to_onnx(
                model_name=model_name,
                task_type="sentiment",
                output_path=self.engine_dir / "sentiment_roberta.onnx"
            )
            
            if not onnx_path:
                logger.error("❌ ONNX conversion failed")
                return False
            
            # Step 2: Build TensorRT engine from ONNX
            success = self._build_tensorrt_engine_from_onnx(
                onnx_path=onnx_path,
                engine_path=engine_path,
                num_classes=3,  # negative, neutral, positive
                task_name="sentiment"
            )
            
            if success:
                self.compiled_engines['sentiment'] = str(engine_path)
                logger.info(f"✅ Native sentiment engine compiled: {engine_path}")
                return True
            else:
                logger.error("❌ Native sentiment engine compilation failed")
                return False
                
        except Exception as e:
            logger.error(f"❌ Sentiment model compilation error: {e}")
            return False
    
    def compile_bias_model(self) -> bool:
        """Compile BERT bias detection model to native TensorRT engine"""
        logger.info("🔧 Compiling BERT bias model to native TensorRT")
        
        model_name = "unitary/toxic-bert"
        engine_path = self.engine_dir / "native_bias_bert.engine"
        
        try:
            # Step 1: Convert HuggingFace model to ONNX
            onnx_path = self._convert_to_onnx(
                model_name=model_name,
                task_type="bias",
                output_path=self.engine_dir / "bias_bert.onnx"
            )
            
            if not onnx_path:
                logger.error("❌ ONNX conversion failed")
                return False
            
            # Step 2: Build TensorRT engine from ONNX
            success = self._build_tensorrt_engine_from_onnx(
                onnx_path=onnx_path,
                engine_path=engine_path,
                num_classes=2,  # not_toxic, toxic
                task_name="bias"
            )
            
            if success:
                self.compiled_engines['bias'] = str(engine_path)
                logger.info(f"✅ Native bias engine compiled: {engine_path}")
                return True
            else:
                logger.error("❌ Native bias engine compilation failed")
                return False
                
        except Exception as e:
            logger.error(f"❌ Bias model compilation error: {e}")
            return False
    
    def _convert_to_onnx(self, model_name: str, task_type: str, output_path: "Path" = None) -> Optional[str]:
        """Convert HuggingFace model to ONNX format with proper input handling"""
        try:
            logger.info(f"📄 Converting {model_name} to ONNX format")
            
            # Load model and tokenizer
            if task_type == "sentiment":
                tokenizer = AutoTokenizer.from_pretrained(model_name)
                model = AutoModelForSequenceClassification.from_pretrained(model_name)
            elif task_type == "bias":
                tokenizer = AutoTokenizer.from_pretrained(model_name)
                model = AutoModelForSequenceClassification.from_pretrained(model_name)
            else:
                raise ValueError(f"Unsupported task type: {task_type}")
            
            model.eval()
            
            # Get model's expected sequence length from config
            max_length = getattr(model.config, 'max_position_embeddings', 512)
            if hasattr(tokenizer, 'model_max_length'):
                max_length = min(max_length, tokenizer.model_max_length)
            
            # Handle special cases for different model architectures
            if 'roberta' in model_name.lower():
                max_length = min(max_length, 514)  # RoBERTa typically uses 514
            elif 'bert' in model_name.lower():
                max_length = min(max_length, 512)  # BERT uses 512
            
            logger.info(f"🔧 Using sequence length: {max_length} for {model_name}")
            
            # Create dummy input with model-specific sequence length
            dummy_text = "This is a sample text for ONNX conversion. " * 10  # Longer text
            dummy_input = tokenizer(
                dummy_text,
                return_tensors="pt",
                max_length=max_length,
                padding="max_length",
                truncation=True
            )
            
            # Verify input shapes match model expectations
            logger.info(f"📐 Input shapes: {[(k, v.shape) for k, v in dummy_input.items()]}")
            
            # Set up ONNX export path
            if output_path:
                onnx_path = str(output_path)
            else:
                onnx_path = os.path.join(self.engines_dir, f"{task_type}_model.onnx")
            
            # Export to ONNX with dynamic axes
            torch.onnx.export(
                model,
                tuple(dummy_input.values()),
                onnx_path,
                export_params=True,
                opset_version=14,
                do_constant_folding=True,
                input_names=['input_ids', 'attention_mask'],
                output_names=['logits'],
                dynamic_axes={
                    'input_ids': {0: 'batch_size'},
                    'attention_mask': {0: 'batch_size'},
                    'logits': {0: 'batch_size'}
                },
                verbose=False
            )
            
            logger.info(f"✅ ONNX model saved: {onnx_path}")
            return onnx_path
            
        except Exception as e:
            logger.error(f"❌ ONNX conversion failed: {e}")
            logger.error(f"📋 Model config: {getattr(model, 'config', 'No config available')}")
            return None
    
    def _build_tensorrt_engine_from_onnx(self, onnx_path: Path, engine_path: Path, 
                                       num_classes: int, task_name: str) -> bool:
        """Build optimized TensorRT engine from ONNX model"""
        try:
            logger.info(f"⚡ Building native TensorRT engine for {task_name}")
            
            import tensorrt as trt
            import pycuda.driver as cuda
            import pycuda.autoinit
            
            # Create TensorRT logger and builder
            TRT_LOGGER = trt.Logger(trt.Logger.INFO)
            builder = trt.Builder(TRT_LOGGER)
            
            # Create network and parser
            network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))
            parser = trt.OnnxParser(network, TRT_LOGGER)
            
            # Parse ONNX model
            with open(onnx_path, 'rb') as model_file:
                if not parser.parse(model_file.read()):
                    logger.error("❌ ONNX parsing failed")
                    for error in range(parser.num_errors):
                        logger.error(f"ONNX Error: {parser.get_error(error)}")
                    return False
            
            # Configure builder
            config = builder.create_builder_config()
            
            # Set memory limit using the new API (TensorRT 8.5+)
            try:
                # Try new API first (TensorRT 8.5+)
                config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, self.optimization_config['workspace_size'])
            except AttributeError:
                # Fallback to old API
                config.max_workspace_size = self.optimization_config['workspace_size']
            
            # Enable optimizations based on GPU capabilities
            if self.tensor_cores_available and self.optimization_config['precision'] == 'fp16':
                config.set_flag(trt.BuilderFlag.FP16)
                logger.info("✅ FP16 precision enabled for Tensor Cores")
            elif self.optimization_config['precision'] == 'int8':
                config.set_flag(trt.BuilderFlag.INT8)
                logger.info("✅ INT8 quantization enabled")
                # Note: INT8 requires calibration dataset for production
            
            # Get actual input shapes from the ONNX model
            input0 = network.get_input(0)  # input_ids
            input1 = network.get_input(1)  # attention_mask
            
            # Determine sequence length from the ONNX model
            actual_seq_len = input0.shape[1]
            logger.info(f"📐 Detected sequence length from ONNX: {actual_seq_len}")
            
            # Create optimization profile for dynamic batch size with fixed sequence length
            profile = builder.create_optimization_profile()
            
            # Set dynamic input shapes - only batch size is dynamic, sequence length is fixed
            min_shape = (1, actual_seq_len)
            opt_shape = (self.optimization_config['max_batch_size'] // 2, actual_seq_len)
            max_shape = (self.optimization_config['max_batch_size'], actual_seq_len)
            
            profile.set_shape('input_ids', min_shape, opt_shape, max_shape)
            profile.set_shape('attention_mask', min_shape, opt_shape, max_shape)
            
            # Handle third input for BERT models (token_type_ids)
            if network.num_inputs > 2:
                input2 = network.get_input(2)  # token_type_ids
                input2_shape = input2.shape
                
                # Check if the third input has dynamic batch dimension
                if input2_shape[0] == -1:  # Dynamic batch dimension
                    profile.set_shape(input2.name, min_shape, opt_shape, max_shape)
                    logger.info(f"📐 Added dynamic {input2.name} profile")
                else:
                    # Static shape - use actual dimensions
                    static_shape = (input2_shape[0], input2_shape[1])
                    profile.set_shape(input2.name, static_shape, static_shape, static_shape)
                    logger.info(f"📐 Added static {input2.name} profile: {static_shape}")
            
            config.add_optimization_profile(profile)
            
            logger.info(f"🎯 Optimization profile: min={min_shape}, opt={opt_shape}, max={max_shape}")
            
            # Build engine
            logger.info("⚡ Building optimized TensorRT engine... (this may take several minutes)")
            start_time = time.time()
            
            serialized_engine = builder.build_serialized_network(network, config)
            
            if serialized_engine is None:
                logger.error("❌ TensorRT engine building failed")
                return False
            
            build_time = time.time() - start_time
            logger.info(f"✅ Engine built in {build_time:.1f} seconds")
            
            # Save engine to file
            with open(engine_path, 'wb') as f:
                f.write(serialized_engine)
            
            # Save engine metadata
            metadata = {
                'model_name': task_name,
                'num_classes': num_classes,
                'max_batch_size': self.optimization_config['max_batch_size'],
                'sequence_length': actual_seq_len,
                'precision': self.optimization_config['precision'],
                'build_time': build_time,
                'created_at': time.strftime('%Y-%m-%d %H:%M:%S'),
                'tensorrt_version': trt.__version__
            }
            
            metadata_path = engine_path.with_suffix('.json')
            with open(metadata_path, 'w') as f:
                json.dump(metadata, f, indent=2)
            
            logger.info(f"✅ Native TensorRT engine compiled successfully!")
            logger.info(f"   Engine: {engine_path}")
            logger.info(f"   Metadata: {metadata_path}")
            logger.info(f"   Max Batch Size: {self.optimization_config['max_batch_size']}")
            logger.info(f"   Sequence Length: {actual_seq_len}")
            logger.info(f"   Precision: {self.optimization_config['precision'].upper()}")
            
            return True
            
        except Exception as e:
            logger.error(f"❌ TensorRT engine building failed: {e}")
            return False
    
    def compile_all_models(self) -> Dict[str, bool]:
        """Compile all models to native TensorRT engines"""
        logger.info("🚀 Starting full native TensorRT compilation")
        
        results = {}
        
        # Compile sentiment model
        logger.info("\n" + "="*60)
        logger.info("📊 COMPILING SENTIMENT ANALYSIS MODEL")
        logger.info("="*60)
        results['sentiment'] = self.compile_sentiment_model()
        
        # Compile bias model
        logger.info("\n" + "="*60)
        logger.info("🔍 COMPILING BIAS DETECTION MODEL")
        logger.info("="*60)
        results['bias'] = self.compile_bias_model()
        
        # Summary
        logger.info("\n" + "="*60)
        logger.info("🎯 NATIVE TENSORRT COMPILATION SUMMARY")
        logger.info("="*60)
        
        successful = sum(results.values())
        total = len(results)
        
        for model, success in results.items():
            status = "✅ SUCCESS" if success else "❌ FAILED"
            logger.info(f"  {model.capitalize()}: {status}")
        
        if successful == total:
            logger.info(f"\n🎉 ALL MODELS COMPILED SUCCESSFULLY!")
            logger.info(f"🚀 Ready for 2-4x performance improvement!")
            logger.info(f"   Target: 300-600 articles/sec")
            logger.info(f"   Engines: {list(self.compiled_engines.keys())}")
        else:
            logger.warning(f"\n⚠️  {successful}/{total} models compiled successfully")
            if successful > 0:
                logger.info(f"   Working engines: {list(self.compiled_engines.keys())}")
        
        return results
    
    def get_compiled_engines(self) -> Dict[str, str]:
        """Get dictionary of compiled engine paths"""
        return self.compiled_engines.copy()
    
    def validate_engines(self) -> Dict[str, bool]:
        """Validate compiled engines"""
        logger.info("🔍 Validating compiled TensorRT engines")
        
        validation_results = {}
        
        for task, engine_path in self.compiled_engines.items():
            try:
                import tensorrt as trt
                
                # Create runtime and load engine
                TRT_LOGGER = trt.Logger(trt.Logger.WARNING)
                runtime = trt.Runtime(TRT_LOGGER)
                
                with open(engine_path, 'rb') as f:
                    engine_data = f.read()
                
                engine = runtime.deserialize_cuda_engine(engine_data)
                
                if engine is None:
                    validation_results[task] = False
                    logger.error(f"❌ {task} engine validation failed")
                else:
                    # Get engine info
                    num_bindings = engine.num_bindings
                    max_batch_size = engine.max_batch_size
                    
                    validation_results[task] = True
                    logger.info(f"✅ {task} engine validated:")
                    logger.info(f"   Bindings: {num_bindings}")
                    logger.info(f"   Max Batch Size: {max_batch_size}")
                
            except Exception as e:
                validation_results[task] = False
                logger.error(f"❌ {task} engine validation error: {e}")
        
        return validation_results


def main():
    """Main compilation process"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Native TensorRT Engine Compiler")
    parser.add_argument("--sentiment", action="store_true", help="Compile sentiment model only")
    parser.add_argument("--bias", action="store_true", help="Compile bias model only")
    parser.add_argument("--validate", action="store_true", help="Validate existing engines")
    parser.add_argument("--precision", choices=['fp16', 'int8', 'fp32'], default='fp16',
                       help="Precision mode (default: fp16)")
    parser.add_argument("--max-batch-size", type=int, default=100,
                       help="Maximum batch size (default: 100)")
    
    args = parser.parse_args()
    
    # Initialize compiler
    compiler = NativeTensorRTCompiler()
    
    # Update configuration
    compiler.optimization_config['precision'] = args.precision
    compiler.optimization_config['max_batch_size'] = args.max_batch_size
    
    if args.validate:
        # Validate existing engines
        validation_results = compiler.validate_engines()
        if all(validation_results.values()):
            logger.info("🎉 All engines validated successfully!")
        else:
            logger.error("❌ Some engines failed validation")
        return
    
    # Compile models
    if args.sentiment:
        results = {'sentiment': compiler.compile_sentiment_model()}
    elif args.bias:
        results = {'bias': compiler.compile_bias_model()}
    else:
        results = compiler.compile_all_models()
    
    # Final status
    if all(results.values()):
        logger.info("\n🚀 READY FOR MAXIMUM PERFORMANCE!")
        logger.info("   Run the TensorRT accelerated analyst to see 2-4x speedup!")
    else:
        logger.error("\n❌ Some compilations failed - check logs above")


if __name__ == "__main__":
    main()
