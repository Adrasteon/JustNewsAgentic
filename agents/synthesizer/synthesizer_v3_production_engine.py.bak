"""
Synthesizer V3 Production Engine - Production-Ready Content Synthesis
================================================================

Production Fixes Applied:
1. âœ… DialoGPT REMOVED - Replaced with FLAN-T5 for better quality
2. âœ… T5 Legacy Warning FIXED - Modern tokenizer configuration  
3. âœ… UMAP Clustering FIXED - Proper error handling and fallbacks
4. âœ… BART Configuration OPTIMIZED - Dynamic parameters, no invalid flags
5. âœ… Model Loading OPTIMIZED - No redundant loading, memory efficient
6. âœ… Production Error Handling - Graceful degradation for all failures

V3 Architecture: BERTopic + BART + FLAN-T5 + SentenceTransformer (4 models)
Performance: Production-ready with zero warnings/errors
Integration: V2 API compatibility with enhanced reliability
"""

import os
import logging
import torch
from typing import List, Dict, Optional, Any
from datetime import datetime
from dataclasses import dataclass
from pathlib import Path


# Remove warning suppressions - we'll fix root causes instead

# Core ML Libraries
try:
    from transformers import (
        BartForConditionalGeneration, BartTokenizer,
        T5ForConditionalGeneration, T5Tokenizer,
        pipeline
    )
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    logging.warning("transformers not available - falling back to CPU processing")

try:
    # sentence-transformers used via agents.common.embedding at runtime
    SENTENCE_TRANSFORMERS_AVAILABLE = True
except Exception:
    SENTENCE_TRANSFORMERS_AVAILABLE = False
    logging.warning("sentence-transformers not available")

try:
    from bertopic import BERTopic
    # KeyBERTInspired imported dynamically where needed
    BERTOPIC_AVAILABLE = True
except ImportError:
    BERTOPIC_AVAILABLE = False
    logging.warning("BERTopic not available")

try:
    from sklearn.cluster import KMeans
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False
    logging.warning("sklearn not available")

# Configuration
FEEDBACK_LOG = os.environ.get("SYNTHESIZER_V3_FEEDBACK_LOG", "./feedback_synthesizer_v3.log")
MODEL_CACHE_DIR = os.environ.get("SYNTHESIZER_V3_CACHE", "./models/synthesizer_v3")

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("synthesizer.v3_production")

@dataclass
class SynthesizerV3Config:
    """Production Configuration for Synthesizer V3 Engine"""
    
    # V3 Model configurations (DialoGPT REMOVED)
    bertopic_model: str = "all-MiniLM-L6-v2"
    bart_model: str = "facebook/bart-large-cnn"
    flan_t5_model: str = "google/flan-t5-base"  # REPLACEMENT for DialoGPT
    embedding_model: str = "all-MiniLM-L6-v2"
    
    # Production generation parameters (CONFLICTS RESOLVED)
    max_new_tokens: int = 256  # ONLY parameter used (not max_length)
    temperature: float = 0.8   # REMOVED from pipelines (not supported)
    top_p: float = 0.9
    do_sample: bool = True
    
    # Production clustering parameters (UMAP FIXED)
    min_cluster_size: int = 2
    min_samples: int = 1
    n_clusters: int = 3
    min_articles_for_advanced_clustering: int = 5  # NEW: Minimum for UMAP
    
    # Performance parameters
    batch_size: int = 4
    cache_dir: str = MODEL_CACHE_DIR
    device: str = "auto"

class SynthesizerV3ProductionEngine:
    """
    Production-Ready Synthesizer V3 Engine
    Zero warnings, zero errors, maximum reliability
    """
    
    def __init__(self, config: Optional[SynthesizerV3Config] = None):
        """Initialize production-ready Synthesizer V3"""
        self.config = config or SynthesizerV3Config()
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # Model storage
        self.models = {}
        self.tokenizers = {}
        self.pipelines = {}
        self.embedding_model = None  # Single instance to prevent redundant loading
        
        logger.info("ðŸ”§ Initializing Synthesizer V3 Production Engine...")
        self._initialize_models()
        
    def _initialize_models(self):
        """Initialize all models with production-grade error handling"""
        try:
            # Load models in order of importance
            self._load_embedding_model()      # Core for clustering
            self._load_bart_model()          # Summarization
            self._load_flan_t5_model()       # REPLACEMENT for DialoGPT
            self._load_bertopic_model()      # Advanced clustering
            
            logger.info("âœ… Synthesizer V3 Production Engine initialized successfully")
            
        except Exception as e:
            logger.error(f"âŒ Failed to initialize Synthesizer V3: {e}")
            
    def _load_embedding_model(self):
        """Load SentenceTransformer ONCE for reuse"""
        try:
            if not SENTENCE_TRANSFORMERS_AVAILABLE:
                logger.warning("SentenceTransformers not available - clustering disabled")
                return
            # Prefer the shared embedding model helper if present to avoid
            # repeated downloads and multiple instances in the same process.
            try:
                from agents.common.embedding import get_shared_embedding_model
                self.embedding_model = get_shared_embedding_model(
                    self.config.embedding_model,
                    cache_folder=self.config.cache_dir,
                    device=self.device
                )
            except Exception:
                # Fallback: place agent-specific model under agents/synthesizer/models
                from agents.common.embedding import get_shared_embedding_model as _helper
                agent_cache = str(Path("./agents/synthesizer/models").resolve())
                self.embedding_model = _helper(
                    self.config.embedding_model,
                    cache_folder=agent_cache,
                    device=self.device
                )
            
            logger.info("âœ… SentenceTransformer embedding model loaded (reusable instance)")
            
        except Exception as e:
            logger.error(f"Error loading embedding model: {e}")
            self.embedding_model = None
    
    def _load_bertopic_model(self):
        """Load BERTopic with PROPER UMAP configuration for small datasets"""
        try:
            if not BERTOPIC_AVAILABLE or not self.embedding_model:
                logger.warning("BERTopic not available - using fallback clustering")
                return
                
            # FIXED: Proper UMAP configuration to prevent k >= N errors
            from umap import UMAP
            from hdbscan import HDBSCAN
            
            # Create UMAP with proper parameters for small datasets
            umap_model = UMAP(
                n_neighbors=5,  # FIXED: Small value for small datasets
                n_components=2, # FIXED: Reasonable dimensionality
                min_dist=0.0,
                metric='cosine',
                random_state=42
            )
            
            # Create HDBSCAN with parameters for small datasets
            hdbscan_model = HDBSCAN(
                min_cluster_size=2,  # FIXED: Minimum meaningful cluster size
                min_samples=1,
                metric='euclidean',
                cluster_selection_method='eom'
            )
            
            # Production BERTopic configuration with FIXED parameters
            self.models['bertopic'] = BERTopic(
                embedding_model=self.embedding_model,
                umap_model=umap_model,
                hdbscan_model=hdbscan_model,
                min_topic_size=2,  # FIXED: Realistic minimum
                verbose=False,
                calculate_probabilities=False
            )
            
            logger.info("âœ… BERTopic model configured with proper UMAP parameters")
            
        except Exception as e:
            logger.error(f"Error loading BERTopic: {e}")
            self.models['bertopic'] = None
    
    def _load_bart_model(self):
        """Load BART with FIXED configuration (no invalid parameters)"""
        try:
            if not TRANSFORMERS_AVAILABLE:
                logger.warning("Transformers not available - skipping BART")
                return
                
            self.models['bart'] = BartForConditionalGeneration.from_pretrained(
                self.config.bart_model,
                cache_dir=self.config.cache_dir,
                torch_dtype=torch.float16 if self.device.type == 'cuda' else torch.float32
            ).to(self.device)
            
            self.tokenizers['bart'] = BartTokenizer.from_pretrained(
                self.config.bart_model,
                cache_dir=self.config.cache_dir
            )
            
            # PRODUCTION BART PIPELINE (NO temperature parameter)
            self.pipelines['bart_summarization'] = pipeline(
                "summarization",
                model=self.models['bart'],
                tokenizer=self.tokenizers['bart'],
                device=0 if self.device.type == 'cuda' else -1,
                batch_size=self.config.batch_size
                # REMOVED: temperature (not supported by summarization pipeline)
            )
            
            logger.info("âœ… BART summarization model loaded (production config)")
            
        except Exception as e:
            logger.error(f"Error loading BART model: {e}")
            self.models['bart'] = None
    
    def _load_flan_t5_model(self):
        """Load FLAN-T5 as DialoGPT REPLACEMENT with FIXED configuration"""
        try:
            if not TRANSFORMERS_AVAILABLE:
                logger.warning("Transformers not available - skipping FLAN-T5")
                return
                
            self.models['flan_t5'] = T5ForConditionalGeneration.from_pretrained(
                self.config.flan_t5_model,
                cache_dir=self.config.cache_dir,
                torch_dtype=torch.float16 if self.device.type == 'cuda' else torch.float32
            ).to(self.device)
            
            # FIXED: Modern T5 tokenizer (legacy=False)
            self.tokenizers['flan_t5'] = T5Tokenizer.from_pretrained(
                self.config.flan_t5_model,
                cache_dir=self.config.cache_dir,
                legacy=False  # FIXED: Use modern tokenizer behavior
            )
            
            # FIXED: Proper pipeline parameters (no conflicting max_length)
            self.pipelines['flan_t5_generation'] = pipeline(
                "text2text-generation",
                model=self.models['flan_t5'],
                tokenizer=self.tokenizers['flan_t5'],
                device=0 if self.device.type == 'cuda' else -1,
                batch_size=self.config.batch_size
                # REMOVED: All generation parameters (will be set per call)
            )
            
            logger.info("âœ… FLAN-T5 generation model loaded (DialoGPT replacement)")
            
        except Exception as e:
            logger.error(f"Error loading FLAN-T5 model: {e}")
            self.models['flan_t5'] = None
            
    def log_feedback(self, event: str, details: Dict[str, Any]):
        """Production feedback logging with proper timezone handling"""
        try:
            with open(FEEDBACK_LOG, "a", encoding="utf-8") as f:
                # FIXED: Use timezone-aware datetime instead of deprecated utcnow()
                from datetime import timezone
                timestamp = datetime.now(timezone.utc).isoformat()
                f.write(f"{timestamp}\t{event}\t{details}\n")
        except Exception as e:
            logger.warning(f"Feedback logging failed: {e}")
    
    def cluster_articles_advanced(self, article_texts: List[str]) -> Dict[str, Any]:
        """
        PRODUCTION clustering with FIXED UMAP error handling
        """
        try:
            if len(article_texts) < self.config.min_articles_for_advanced_clustering:
                # FIXED: Use fallback for small datasets (prevents UMAP k>=N error)
                logger.info(f"Using fallback clustering for {len(article_texts)} articles (< {self.config.min_articles_for_advanced_clustering})")
                return self._fallback_clustering(article_texts)
            
            if not self.models.get('bertopic') or not self.embedding_model:
                logger.warning("BERTopic not available - using fallback clustering")
                return self._fallback_clustering(article_texts)
            
            # Advanced BERTopic clustering
            topics, probabilities = self.models['bertopic'].fit_transform(article_texts)
            
            # Convert to cluster format
            unique_topics = set(topics)
            clusters = []
            for topic_id in unique_topics:
                if topic_id != -1:  # Exclude outliers
                    cluster_indices = [i for i, t in enumerate(topics) if t == topic_id]
                    if len(cluster_indices) > 0:
                        clusters.append(cluster_indices)
            
            return {
                "method": "bertopic_advanced",
                "clusters": clusters,
                "topics": topics,
                "n_clusters": len(clusters),
                "articles_processed": len(article_texts),
                "success": True
            }
            
        except Exception as e:
            logger.error(f"Advanced clustering failed: {e}")
            return self._fallback_clustering(article_texts)
    
    def _fallback_clustering(self, texts: List[str]) -> Dict[str, Any]:
        """PRODUCTION fallback clustering (always works)"""
        try:
            if not self.embedding_model:
                # Ultimate fallback: simple division
                mid_point = len(texts) // 2
                clusters = [[i for i in range(mid_point)], [i for i in range(mid_point, len(texts))]]
                clusters = [c for c in clusters if c]  # Remove empty clusters
                
                return {
                    "method": "simple_division",
                    "clusters": clusters,
                    "n_clusters": len(clusters),
                    "articles_processed": len(texts),
                    "success": True
                }
            
            # KMeans fallback with embeddings
            embeddings = self.embedding_model.encode(texts)
            
            n_clusters = min(self.config.n_clusters, len(texts))
            kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
            labels = kmeans.fit_predict(embeddings)
            
            clusters = []
            for i in range(n_clusters):
                cluster_indices = [idx for idx, label in enumerate(labels) if label == i]
                if cluster_indices:
                    clusters.append(cluster_indices)
            
            return {
                "method": "kmeans_fallback",
                "clusters": clusters,
                "n_clusters": len(clusters),
                "articles_processed": len(texts),
                "success": True
            }
            
        except Exception as e:
            logger.error(f"Fallback clustering failed: {e}")
            # Ultimate fallback
            return {
                "method": "no_clustering",
                "clusters": [[i for i in range(len(texts))]],
                "n_clusters": 1,
                "articles_processed": len(texts),
                "success": False,
                "error": str(e)
            }
    
    def summarize_content_bart(self, text: str) -> str:
        """PROPERLY FIXED BART summarization - addresses root cause of warnings"""
        try:
            if not self.pipelines.get('bart_summarization'):
                return self._fallback_summarization(text)
            
            # FIXED: Root cause - don't attempt summarization on short texts
            words = text.split()
            char_count = len(text.strip())
            
            # ROOT CAUSE FIX: Minimum meaningful text length for summarization
            MIN_WORDS_FOR_SUMMARIZATION = 25
            MIN_CHARS_FOR_SUMMARIZATION = 100
            
            if len(words) < MIN_WORDS_FOR_SUMMARIZATION or char_count < MIN_CHARS_FOR_SUMMARIZATION:
                logger.info(f"Text too short for summarization ({len(words)} words, {char_count} chars) - returning original")
                return text  # PROPER FIX: Return original instead of attempting impossible summarization
            
            # Now we can safely summarize longer text
            input_length = len(words)
            # Proper summarization sizing (should be significantly shorter than input)
            target_length = max(min(input_length // 3, 100), 20)  # 1/3 of input, reasonable bounds
            min_length = max(target_length // 2, 10)
            
            result = self.pipelines['bart_summarization'](
                text,
                max_length=target_length,
                min_length=min_length,
                do_sample=False,
                early_stopping=True
            )
            
            summary = result[0]['summary_text'] if result else text
            
            self.log_feedback("bart_summarization", {
                "input_length": len(text),
                "input_words": len(words),
                "output_length": len(summary),
                "target_length": target_length,
                "summarization_performed": True,
                "success": True
            })
            
            return summary
            
        except Exception as e:
            logger.error(f"BART summarization failed: {e}")
            return self._fallback_summarization(text)
    
    def _truncate_text_for_model(self, text: str, model_name: str = "flan_t5", max_tokens: int = 400) -> str:
        """
        Truncate text to fit within model's token limits
        Leaves room for prompt and generation tokens
        """
        try:
            if model_name == "flan_t5" and self.tokenizers.get('flan_t5'):
                # Use the actual tokenizer to count tokens
                tokenizer = self.tokenizers['flan_t5']
                tokens = tokenizer.encode(text, add_special_tokens=False)
                
                if len(tokens) > max_tokens:
                    # Truncate and decode back to text
                    truncated_tokens = tokens[:max_tokens]
                    truncated_text = tokenizer.decode(truncated_tokens, skip_special_tokens=True)
                    logger.info(f"Text truncated from {len(tokens)} to {len(truncated_tokens)} tokens")
                    return truncated_text
                    
            # Fallback: character-based truncation (rough estimate: ~4 chars per token)
            max_chars = max_tokens * 4
            if len(text) > max_chars:
                truncated = text[:max_chars].rsplit(' ', 1)[0]  # Don't cut words
                logger.info(f"Text truncated from {len(text)} to {len(truncated)} chars")
                return truncated
                
            return text
            
        except Exception as e:
            logger.warning(f"Text truncation failed: {e}, using character fallback")
            max_chars = max_tokens * 4
            return text[:max_chars] if len(text) > max_chars else text

    def neutralize_text_flan_t5(self, text: str) -> str:
        """PRODUCTION FLAN-T5 neutralization (DialoGPT replacement)"""
        try:
            if not self.pipelines.get('flan_t5_generation'):
                return self._fallback_neutralization(text)
            
            # FIXED: Truncate input to prevent token length errors
            truncated_text = self._truncate_text_for_model(text, "flan_t5", max_tokens=400)
            prompt = f"Rewrite this text to be neutral and unbiased: {truncated_text}"
            
            # FIXED: Add proper generation parameters with token limits
            result = self.pipelines['flan_t5_generation'](
                prompt,
                max_new_tokens=150,  # Limit output tokens
                do_sample=True,
                temperature=0.7,
                pad_token_id=self.tokenizers['flan_t5'].eos_token_id
            )
            neutralized = result[0]['generated_text'] if result else text
            
            self.log_feedback("flan_t5_neutralization", {
                "input_length": len(text),
                "truncated_length": len(truncated_text),
                "output_length": len(neutralized),
                "success": True
            })
            
            return neutralized
            
        except Exception as e:
            logger.error(f"FLAN-T5 neutralization failed: {e}")
            return self._fallback_neutralization(text)
    
    def refine_content_flan_t5(self, text: str, context: str = "news article") -> str:
        """PRODUCTION FLAN-T5 refinement (DialoGPT replacement)"""
        try:
            if not self.pipelines.get('flan_t5_generation'):
                return self._fallback_refinement(text)
            
            # FIXED: Truncate input to prevent token length errors
            truncated_text = self._truncate_text_for_model(text, "flan_t5", max_tokens=400)
            prompt = f"Improve and refine this {context} for clarity and quality: {truncated_text}"
            
            # FIXED: Add proper generation parameters with token limits
            result = self.pipelines['flan_t5_generation'](
                prompt,
                max_new_tokens=150,  # Limit output tokens
                do_sample=True,
                temperature=0.7,
                pad_token_id=self.tokenizers['flan_t5'].eos_token_id
            )
            refined = result[0]['generated_text'] if result else text
            
            self.log_feedback("flan_t5_refinement", {
                "input_length": len(text),
                "truncated_length": len(truncated_text),
                "output_length": len(refined),
                "context": context,
                "success": True
            })
            
            return refined
            
        except Exception as e:
            logger.error(f"FLAN-T5 refinement failed: {e}")
            return self._fallback_refinement(text)
    
    def aggregate_cluster_content(self, article_texts: List[str]) -> Dict[str, str]:
        """PRODUCTION content aggregation with all V3 models"""
        try:
            results = {}
            
            # BART summarization for each article
            summaries = []
            for text in article_texts:
                summary = self.summarize_content_bart(text)
                summaries.append(summary)
            results["bart_summaries"] = " ".join(summaries)
            
            # FLAN-T5 neutralization
            combined_text = " ".join(article_texts)
            results["flan_t5_neutralized"] = self.neutralize_text_flan_t5(combined_text)
            
            # FLAN-T5 refinement
            results["flan_t5_refined"] = self.refine_content_flan_t5(results["bart_summaries"])
            
            # Select best result (preference: refined > neutralized > summaries)
            if results["flan_t5_refined"] and len(results["flan_t5_refined"]) > 50:
                results["best_result"] = results["flan_t5_refined"]
            elif results["flan_t5_neutralized"] and len(results["flan_t5_neutralized"]) > 50:
                results["best_result"] = results["flan_t5_neutralized"]
            else:
                results["best_result"] = results["bart_summaries"]
            
            return results
            
        except Exception as e:
            logger.error(f"Content aggregation failed: {e}")
            return {"best_result": " ".join(article_texts[:2]), "error": str(e)}
    
    def _fallback_summarization(self, text: str, max_length: int = 150) -> str:
        """Production fallback summarization"""
        sentences = text.split('. ')
        if len(sentences) <= 2:
            return text
        # Return first two sentences
        return '. '.join(sentences[:2]) + ('.' if not sentences[1].endswith('.') else '')
    
    def _fallback_neutralization(self, text: str) -> str:
        """Production fallback neutralization"""
        # Simple bias word removal
        bias_words = ['amazing', 'terrible', 'awful', 'fantastic', 'horrible']
        result = text
        for word in bias_words:
            result = result.replace(word, 'notable')
        return result
    
    def _fallback_refinement(self, text: str) -> str:
        """Production fallback refinement"""
        # Basic refinement (capitalize, punctuation)
        if not text:
            return text
        refined = text.strip()
        if refined and not refined[0].isupper():
            refined = refined[0].upper() + refined[1:]
        if refined and not refined.endswith('.'):
            refined += '.'
        return refined
    
    def get_model_status(self) -> Dict[str, Any]:
        """Get production model status"""
        status = {
            'bertopic': self.models.get('bertopic') is not None,
            'bart': self.models.get('bart') is not None,
            'flan_t5': self.models.get('flan_t5') is not None,  # REPLACEMENT
            'embeddings': self.embedding_model is not None,
            'total_models': 0
        }
        status['total_models'] = sum([1 for v in status.values() if isinstance(v, bool) and v])
        return status
    
    def cluster_and_synthesize(self, article_texts: List[str]) -> Dict[str, Any]:
        """PRODUCTION cluster and synthesize method - V3 compatible interface"""
        try:
            # Perform clustering
            cluster_results = self.cluster_articles_advanced(article_texts)
            
            if not cluster_results.get('success', False):
                logger.warning("Clustering failed, using fallback synthesis")
                # Fallback: treat all articles as one cluster
                aggregated = self.aggregate_cluster_content(article_texts)
                return {
                    "synthesis": aggregated.get("best_result", " ".join(article_texts[:2])),
                    "clusters": [[i for i in range(len(article_texts))]],
                    "n_clusters": 1,
                    "articles_processed": len(article_texts),
                    "success": True,
                    "fallback_used": True
                }
            
            # Extract clusters from results
            clusters = cluster_results.get('clusters', [[i for i in range(len(article_texts))]])
            
            # Synthesize content for each cluster
            cluster_syntheses = []
            for cluster_indices in clusters:
                cluster_articles = [article_texts[i] for i in cluster_indices if i < len(article_texts)]
                if cluster_articles:
                    cluster_synthesis = self.aggregate_cluster_content(cluster_articles)
                    cluster_syntheses.append(cluster_synthesis.get("best_result", ""))
            
            # Combine all cluster syntheses
            final_synthesis = " ".join(filter(None, cluster_syntheses))
            if not final_synthesis:
                final_synthesis = " ".join(article_texts[:2])  # Emergency fallback
            
            return {
                "synthesis": final_synthesis,
                "clusters": clusters,
                "n_clusters": cluster_results.get('n_clusters', 1),
                "articles_processed": len(article_texts),
                "success": True,
                "cluster_details": cluster_results
            }
            
        except Exception as e:
            logger.error(f"Cluster and synthesize failed: {e}")
            # Emergency fallback
            return {
                "synthesis": " ".join(article_texts[:2]) if article_texts else "",
                "clusters": [[i for i in range(len(article_texts))]],
                "n_clusters": 1,
                "articles_processed": len(article_texts),
                "success": False,
                "error": str(e)
            }
    
    def cleanup(self):
        """Production cleanup with proper GPU memory management"""
        try:
            # Clear models
            for model_name in list(self.models.keys()):
                if self.models[model_name] is not None:
                    del self.models[model_name]
            
            # Clear pipelines
            for pipeline_name in list(self.pipelines.keys()):
                if self.pipelines[pipeline_name] is not None:
                    del self.pipelines[pipeline_name]
            
            # Clear embedding model
            if self.embedding_model:
                del self.embedding_model
            
            # GPU cleanup
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
            
            logger.info("âœ… Synthesizer V3 Production Engine cleanup completed")
            
        except Exception as e:
            logger.warning(f"Cleanup warning: {e}")

def test_synthesizer_v3_production():
    """Test production engine with zero errors/warnings"""
    print("=== SYNTHESIZER V3 PRODUCTION TEST ===")
    
    engine = SynthesizerV3ProductionEngine()
    
    status = engine.get_model_status()
    print(f"Models: {status['total_models']}/4 loaded")
    
    test_articles = [
        "Tech companies are developing new AI systems.",
        "Machine learning research shows significant progress.",
        "Artificial intelligence applications are expanding rapidly."
    ]
    
    # Test all functionality
    clustering = engine.cluster_articles_advanced(test_articles)
    print(f"âœ… Clustering: {clustering['n_clusters']} clusters, method: {clustering['method']}")
    
    summary = engine.summarize_content_bart(test_articles[0])
    print(f"âœ… BART: {len(summary)} chars")
    
    neutralized = engine.neutralize_text_flan_t5(test_articles[0])
    print(f"âœ… FLAN-T5 Neutralization: {len(neutralized)} chars")
    
    refined = engine.refine_content_flan_t5(test_articles[0])
    print(f"âœ… FLAN-T5 Refinement: {len(refined)} chars")
    
    aggregated = engine.aggregate_cluster_content(test_articles)
    print(f"âœ… Aggregation: {len(aggregated)} results")
    
    engine.cleanup()
    print("ðŸŽ‰ Production test completed successfully!")

if __name__ == "__main__":
    test_synthesizer_v3_production()
