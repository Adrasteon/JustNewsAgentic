"""
GPU Cleanup Utilities for PyTorch Models
        try:
            logger.info("üßπ Starting GPU model cleanup...")
            
            # Clean up registered models
            for name, model in list(self._models.items()):
                try:
                    if hasattr(model, 'cpu'):
                        model.cpu()
                    elif hasattr(model, 'model') and hasattr(model.model, 'cpu'):
                        model.model.cpu()
                    del model
                except Exception as e:
                    logger.debug(f"Error cleaning up model {name}: {e}")
            
            # Clear the models dictionary
            self._models.clear() during shutdown by proper GPU memory management
"""
import logging
import signal
import sys
import gc
import atexit
from typing import Any

logger = logging.getLogger(__name__)

class GPUModelManager:
    """
    Context manager for safe GPU model loading and cleanup
    Prevents core dumps by ensuring proper cleanup order
    """
    
    def __init__(self):
        """Initialize GPU model manager"""
        self._models = {}  # Dictionary to store registered models
        self._cleanup_registered = False
        self._signal_handlers_set = False
    
    def register_model(self, name: str, model: Any) -> None:
        """Register a model or pipeline for cleanup"""
        if model is not None:
            self._models[name] = model
            self.setup_cleanup_handlers()
            logger.debug(f"üìù Registered model: {name}")
    
    def register_model_legacy(self, model: Any) -> None:
        """Register a model (legacy single-parameter version)"""
        if model is not None:
            model_id = id(model)
            self._models[f"model_{model_id}"] = model
            self.setup_cleanup_handlers()
            logger.debug(f"üìù Registered model with ID: {model_id}")
    
    def cleanup_all_models(self) -> None:
        """Explicitly cleanup all registered models"""
        try:
            import torch
            
            logger.info("üßπ Starting GPU model cleanup...")
            
            # Clear all registered models
            for name, model in self._models.items():
                if model is not None:
                    try:
                        # Move to CPU first
                        if hasattr(model, 'cpu'):
                            model.cpu()
                        # Delete the model
                        del model
                        logger.debug(f"‚úÖ Model {name} moved to CPU and deleted")
                    except Exception as e:
                        logger.warning(f"‚ö†Ô∏è Error cleaning up model {name}: {e}")
            
            # Clear the models list
            self._models.clear()
            
            # Force garbage collection
            gc.collect()
            
            # Clear CUDA cache if available
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
                logger.info("‚úÖ CUDA cache cleared and synchronized")
            
            logger.info("‚úÖ GPU model cleanup completed successfully")
            
        except ImportError:
            # If torch not available, skip cleanup
            logger.debug("PyTorch not available, skipping GPU cleanup")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Error during GPU cleanup: {e}")
    
    def setup_cleanup_handlers(self) -> None:
        """Setup signal handlers and exit cleanup"""
        if self._cleanup_registered:
            return
            
        # Register cleanup on normal exit
        atexit.register(self.cleanup_all_models)
        
        # Setup signal handlers for graceful shutdown
        if not self._signal_handlers_set:
            def signal_handler(signum, frame):
                logger.info(f"üîî Received signal {signum}, cleaning up...")
                self.cleanup_all_models()
                sys.exit(0)
            
            signal.signal(signal.SIGINT, signal_handler)
            signal.signal(signal.SIGTERM, signal_handler)
            self._signal_handlers_set = True
        
        self._cleanup_registered = True
        logger.debug("‚úÖ GPU cleanup handlers registered")
    
    def __enter__(self):
        self.setup_cleanup_handlers()
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        self.cleanup_all_models()

# Global model manager instance
_global_model_manager = GPUModelManager()

def register_gpu_model(model: Any) -> None:
    """Register a GPU model for automatic cleanup"""
    _global_model_manager.register_model(model)
    _global_model_manager.setup_cleanup_handlers()

def cleanup_gpu_models() -> None:
    """Manually trigger GPU model cleanup"""
    _global_model_manager.cleanup_all_models()

def safe_gpu_context():
    """Context manager for safe GPU operations"""
    return _global_model_manager

def force_clean_exit() -> None:
    """Force a clean exit with proper GPU cleanup"""
    logger.info("üîö Forcing clean exit with GPU cleanup...")
    cleanup_gpu_models()
    
    # Additional cleanup steps
    try:
        import torch
        if torch.cuda.is_available():
            # Clear all CUDA memory
            torch.cuda.empty_cache()
            # Reset CUDA context
            torch.cuda.reset_peak_memory_stats()
            logger.info("‚úÖ CUDA context reset completed")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Error during CUDA context reset: {e}")
    
    # Force garbage collection one more time
    gc.collect()
    
    logger.info("‚úÖ Clean exit preparation completed")

class SafeModelLoader:
    """
    Context manager for loading models safely with automatic cleanup
    """
    
    def __init__(self, model_loader_func, *args, **kwargs):
        self.model_loader_func = model_loader_func
        self.args = args
        self.kwargs = kwargs
        self.model = None
    
    def __enter__(self):
        try:
            self.model = self.model_loader_func(*self.args, **self.kwargs)
            register_gpu_model(self.model)
            return self.model
        except Exception as e:
            logger.error(f"‚ùå Error loading model: {e}")
            raise
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        if self.model is not None:
            try:
                # Move to CPU before cleanup
                if hasattr(self.model, 'cpu'):
                    self.model.cpu()
                del self.model
                self.model = None
                logger.debug("‚úÖ Model cleaned up in context manager")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Error cleaning up model in context: {e}")
