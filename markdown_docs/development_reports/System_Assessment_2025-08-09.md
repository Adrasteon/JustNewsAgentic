# System Assessment and Improvement Plan — 2025-08-09

This document captures a focused assessment of the JustNewsAgentic V4 system and proposes prioritized, actionable improvements for reliability, performance, security, and operations. It synthesizes current context from `README.md` and `.github/copilot-instructions.md`.

## Checklist

- Validate strengths and current state
- Identify gaps across architecture, performance, reliability, security, and ops
- Propose prioritized, actionable improvements (P0/P1/P2)
- Map to existing standards (MCP, GPU, docs, testing)

## Summary

JustNewsAgentic V4 is a production-ready, GPU-accelerated, multi-agent news analysis platform with strong practices around MCP-based communication, TensorRT optimizations, structured logging, and continuous learning. The next phase (V2 engines completion) should prioritize hardening the control plane (MCP Bus), standardizing operational contracts (health, readiness, warmup), strengthening observability and safety (timeouts, backoff, circuit breakers, tracing), and extending TensorRT governance and regression testing across agents.

## Strengths

- Clear multi-agent architecture with MCP Bus and well-defined agent roles/ports.
- Proven GPU acceleration (TensorRT) with strong production metrics and CPU fallbacks.
- Solid engineering standards: type hints, docstrings, error classes, structured logging.
- Documentation discipline with `markdown_docs/` organization and architecture references.
- EWC-based continuous learning integrated with production feedback.

## Improvement Areas (Prioritized)

### P0 — Production reliability and safety

- MCP Bus resilience: timeouts, exponential backoff, circuit breakers, idempotency keys for `/call`; per-agent SLAs and failure budgets.
- Health model: standardize `/health`, `/ready`, `/warmup` across all agents; non-sensitive payloads; warm-up paths to pre-load engines.
- Observability: OpenTelemetry tracing + Prometheus metrics + Grafana dashboards. Track p50/p95/p99 latency, error rates, queue depth, GPU utilization, and memory.
- Schema contracts: centralize Pydantic schemas for tool `args/kwargs` with versioning; add contract tests to protect MCP interfaces.
- Images/dependencies: pin versions per agent; multi-stage Docker builds; slim CUDA bases; SBOM + CVE scanning; enforce `trust_remote_code=False`.

### P1 — Performance and scalability

- Unified GPU utilities: shared `safe_gpu_operation`, memory logging (allocated/reserved), mixed precision control, and consistent cleanup; detect/log leak deltas.
- TensorRT governance: engine cache/versioning, dynamic shapes, calibration artifacts; performance regression tests per model/version.
- Throughput: async I/O, pipelined H2D/D2H, CUDA streams, batch coalescing (16–32 default, 100 peak) with GPU occupancy-driven autoscaling.
- Caching/dedup: article fingerprinting (URL + content hash); result cache with TTL; dedupe at MCP entry to avoid duplicate downstream work.
- Vector storage: confirm pgvector/FAISS usage; embedding column indexes; partitioning/retention policy; connection pooling.

### P1 — Training and model operations

- Model registry and canary: versioned models with metrics; canary rollouts and fast rollback; drift detection (input/label).
- Dataset governance: source/label lineage, license checks; static eval suites with golden baselines for sentiment/bias/fact-checking/synthesis.
- Scheduling/quota: clear training cadence, GPU reservations; guards to maintain 2–3GB production buffer.

### P1 — Security and compliance

- Inter-service auth: mTLS or signed tokens for MCP `/call`; per-agent rate limits; docker-compose resource limits; secrets via env or vault.
- Data privacy: PII redaction in logs; retention windows for raw content and embeddings; DLP scans in CI.
- Supply chain: automated dependency/image scanning (e.g., Trivy/Snyk); reproducible builds.

### P2 — Architecture evolution and ops ergonomics

- Orchestration: evaluate Kubernetes for horizontal scaling and GPU scheduling (NVIDIA device plugin); MIG/affinity as needed.
- Messaging: consider NATS/Kafka for high-throughput data plane with backpressure; keep HTTP tools for control plane.
- Runbooks: per-agent runbooks (failure modes, SLOs, playbooks) and production readiness checklist.
- Ops dashboard: minimal agent/cluster dashboard showing health, queue depth, throughput, GPU mem/temp, and top errors.

## Quick Wins (Next Steps)

- Add OpenTelemetry + Prometheus across MCP Bus and agents; wire Grafana dashboards.
- Implement consistent `/health`, `/ready`, `/warmup` and standardize Pydantic `ToolCall` across services.
- Introduce timeouts, retries with backoff, and circuit breakers on MCP `/call`; log idempotency keys.
- Create shared `gpu_utils` with memory logging and `safe_gpu_operation`; enable mixed precision where safe.
- Pin per-agent requirements; multi-stage Docker builds; add image scanning in CI.
- Add article fingerprinting and result caching; dedupe at the MCP Bus before dispatch.
- Set resource limits in docker-compose; enable rate limits and inter-agent auth.
- Stand up performance regression harness for TensorRT engines; track throughput/p95 in CI.
- Index/optimize vector search; confirm pgvector settings; add retention policies.

## Action Checklist (Trackable)

### P0 — Reliability and Safety

- [ ] MCP Bus resilience: timeouts, retries with exponential backoff, circuit
	breakers, idempotency keys for `/call`.
- [ ] Standardize `/health`, `/ready`, `/warmup` across agents; implement
	non-sensitive payloads and warm-up preloading of engines.
- [ ] Observability stack: OpenTelemetry tracing, Prometheus metrics, Grafana
	dashboards (latency p50/p95/p99, error rates, queue depth, GPU util/mem).
- [ ] Schema contracts: centralize/version Pydantic schemas for tool args/kwargs
	and add contract tests to protect MCP interfaces.
- [ ] Image/dependency hygiene: pin versions, multi-stage Docker builds, slim
	CUDA bases, SBOM and CVE scanning; enforce `trust_remote_code=False`.

### P1 — Performance and Scalability

- [ ] Shared `gpu_utils`: `safe_gpu_operation`, memory logging (allocated/
	reserved), mixed precision control, and leak delta detection.
- [ ] TensorRT governance: engine cache/versioning, dynamic shapes, calibration
	artifacts; performance regression tests per model/version.
- [ ] Throughput improvements: async I/O, pipelined H2D/D2H, CUDA streams,
	batch coalescing (16–32 default, up to 100) with GPU occupancy autoscale.
- [ ] Caching/dedup: article fingerprint (URL + content hash), result cache
	with TTL; dedupe at MCP entry to avoid duplicate downstream work.
- [ ] Vector store: confirm pgvector/FAISS usage; add embedding indexes,
	partitioning/retention; enable connection pooling.

### P1 — Training & Model Operations

- [ ] Model registry + canary rollouts with metrics; fast rollback; drift
	detection (input and label).
- [ ] Dataset governance: lineage and license checks; golden eval suites for
	sentiment/bias/fact-checking/synthesis.
- [ ] Training scheduler/quotas; maintain 2–3GB GPU buffer for production.

### P1 — Security & Compliance

- [ ] Inter-service auth (mTLS or signed tokens) on MCP `/call`; per-agent rate
	limits.
- [ ] docker-compose resource limits; secrets via environment or vault.
- [ ] PII redaction in logs; retention windows for raw content and embeddings;
	CI DLP scans.
- [ ] Supply chain scanning (Trivy/Snyk) and reproducible builds.

### P2 — Architecture & Ops Ergonomics

- [ ] Evaluate Kubernetes for GPU scheduling (NVIDIA device plugin); MIG/
	affinity policies as needed.
- [ ] Assess NATS/Kafka for high-throughput data plane; keep HTTP tools for the
	control plane.
- [ ] Per-agent runbooks, production readiness checklist, SLOs/SLIs and
	playbooks.
- [ ] Ops dashboard with health, queue depth, throughput, GPU mem/temp, top
	errors.

## Documentation and Tests

- Enforce docs placement under `markdown_docs/` only; add runbooks and SLOs under `development_reports/` or `agent_documentation/` as appropriate.
- Add MCP contract tests (schemas and endpoints); performance tests; GPU memory leak checks; update `CHANGELOG.md` with metrics per release.

## Risks to Monitor

- MCP Bus as potential chokepoint: mitigate with HA, rate limiting, and/or message queues.
- GPU fragmentation across agents: mitigate with stream-aware batching and unified allocator policies.
- Training-induced regressions: mitigate with canary, golden sets, and automated rollback.

## References

- Architecture & plans: `markdown_docs/TECHNICAL_ARCHITECTURE.md`, `docs/JustNews_Proposal_V4.md`, `docs/JustNews_Plan_V4.md`
- System overview and metrics: `README.md`
- Engineering standards and patterns: `.github/copilot-instructions.md`

---

Requirements coverage: This document records the system assessment and a prioritized improvement roadmap based on the latest project context (as of 2025-08-09).
